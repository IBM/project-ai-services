apiVersion: v1
kind: Pod
metadata:
  name: "{{ .AppName }}--vllm-server"
  labels:
    ai-services.io/application: "{{ .AppName }}"
    ai-services.io/template: "{{ .AppTemplateName }}"
    ai-services.io/version: "{{ .Version }}"
  annotations:
    ai-services.io/model1: BAAI/bge-reranker-v2-m3
    ai-services.io/model2: ibm-granite/granite-embedding-278m-multilingual
    ai-services.io/model3: ibm-granite/granite-3.3-8b-instruct
    ai-services.io/instruct--sypre-cards: "4"
    ai-services.io/reranker--sypre-cards: "1"
    ai-services.io/embedding--sypre-cards: "1"
spec:
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
    - name: dshm2
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
    - name: dshm3
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
    - name: models
      hostPath:
        path: "/var/lib/ai-services/models"
        type: Directory
  containers:
    - name: instruct
      image: icr.io/ibmaiu_internal/ppc64le/dd2/rhaiis:3.2.5-ci_236
      command: ["/bin/bash"]
      args:
        - "-c"
        - |
          /opt/app-root/spyre_entrypoint.sh \
          --model ${VLLM_MODEL_PATH} \
          -tp ${AIU_WORLD_SIZE} \
          --max-model-len ${MAX_MODEL_LEN} \
          --max-num-seqs ${MAX_BATCH_SIZE} \
          --served-model-name ibm-granite/granite-3.3-8b-instruct --port 8000
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 420
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      env:
        - name: VLLM_MODEL_PATH
          value: "/models/ibm-granite/granite-3.3-8b-instruct"
        - name: AIU_WORLD_SIZE
          value: "4"
        - name: VLLM_SPYRE_USE_CB
          value: "1"
        - name: MAX_MODEL_LEN
          value: "32768"
        - name: MAX_BATCH_SIZE
          value: "32"
        - name: MASTER_PORT
          value: "12355"
        {{- with .env.instruct }}
        {{- range $k, $v := . }}
        - name: {{ $k }}
          value: "{{ $v }}"
          {{- end }}
        {{- end }}
      resources:
        requests:
          podman.io/device=/dev/vfio: 4
          memory: "200Gi"
        limits:
          memory: "200Gi"
      ports:
        - containerPort: 8000
      volumeMounts:
        - mountPath: /models:z
          name: models
          readOnly: true
        - mountPath: /dev/shm
          name: dshm
    - name: embedding
      image: icr.io/ibmaiu_internal/ppc64le/dd2/rhaiis:3.2.5-ci_236
      command: ["/bin/bash"]
      args:
        - "-c"
        - |
          /opt/app-root/spyre_entrypoint.sh \
          --model ${VLLM_MODEL_PATH} \
          -tp ${AIU_WORLD_SIZE} \
          --served-model-name ibm-granite/granite-embedding-278m-multilingual --port 8001
      livenessProbe:
        httpGet:
          path: /health
          port: 8001
        initialDelaySeconds: 120
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      env:
        - name: VLLM_MODEL_PATH
          value: "/models/ibm-granite/granite-embedding-278m-multilingual"
        - name: AIU_WORLD_SIZE
          value: "1"
        - name: VLLM_SPYRE_WARMUP_BATCH_SIZES
          value: "4"
        - name: VLLM_SPYRE_WARMUP_PROMPT_LENS
          value: "512"
        - name: MASTER_PORT
          value: "12356"
        {{- with .env.embedding }}
        {{- range $k, $v := . }}
        - name: {{ $k }}
          value: "{{ $v }}"
          {{- end }}
        {{- end }}
      resources:
        requests:
          podman.io/device=/dev/vfio: 1
          memory: "150Gi"
        limits:
          memory: "150Gi"
      ports:
        - containerPort: 8001
      volumeMounts:
        - mountPath: /models:z
          name: models
          readOnly: true
        - mountPath: /dev/shm
          name: dshm3
    - name: reranker
      image: icr.io/ibmaiu_internal/ppc64le/dd2/rhaiis:3.2.5-ci_236
      command: ["/bin/bash"]
      args:
        - "-c"
        - |
          /opt/app-root/spyre_entrypoint.sh \
          --model ${VLLM_MODEL_PATH} \
          -tp ${AIU_WORLD_SIZE} \
          --served-model-name BAAI/bge-reranker-v2-m3 --port 8002
      livenessProbe:
        httpGet:
          path: /health
          port: 8002
        initialDelaySeconds: 120
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      env:
        - name: VLLM_MODEL_PATH
          value: "/models/BAAI/bge-reranker-v2-m3"
        - name: AIU_WORLD_SIZE
          value: "1"
        - name: VLLM_SPYRE_WARMUP_BATCH_SIZES
          value: "4"
        - name: VLLM_SPYRE_WARMUP_PROMPT_LENS
          value: "512"
        - name: MASTER_PORT
          value: "12356"
        {{- with .env.reranker }}
        {{- range $k, $v := . }}
        - name: {{ $k }}
          value: "{{ $v }}"
          {{- end }}
        {{- end }}
      resources:
        requests:
          podman.io/device=/dev/vfio: 1
          memory: "150Gi"
        limits:
          memory: "150Gi"
      ports:
        - containerPort: 8002
      volumeMounts:
        - mountPath: /models:z
          name: models
          readOnly: true
        - mountPath: /dev/shm
          name: dshm2
